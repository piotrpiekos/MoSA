torch
local-attention # necessary if using local attention head for hybrid